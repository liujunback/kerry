import json
import csv
import sys
import chardet
import re
import os
from io import StringIO
import traceback


def detect_encoding(file_path):
    """æ£€æµ‹æ–‡ä»¶ç¼–ç """
    try:
        with open(file_path, 'rb') as f:
            rawdata = f.read(10000)  # è¯»å–å‰10000å­—èŠ‚ç”¨äºæ£€æµ‹ç¼–ç 
            result = chardet.detect(rawdata)
            return result['encoding'] or 'utf-8'
    except Exception as e:
        print(f"âŒ ç¼–ç æ£€æµ‹å¤±è´¥: {str(e)}")
        return 'utf-8'


def clean_nul_chars(content):
    """ç§»é™¤å†…å®¹ä¸­çš„ç©ºå­—ç¬¦(NUL)"""
    return re.sub(r'\x00', '', content)


def preprocess_file(file_path, encoding):
    """é¢„å¤„ç†æ–‡ä»¶ï¼šç§»é™¤NULå­—ç¬¦å¹¶ä¿®å¤å¸¸è§é—®é¢˜"""
    try:
        # è¯»å–æ•´ä¸ªæ–‡ä»¶å†…å®¹
        with open(file_path, 'rb') as f:
            raw_content = f.read()

        # ç§»é™¤NULå­—ç¬¦
        cleaned_content = clean_nul_chars(raw_content.decode(encoding, errors='replace'))

        # ä¿®å¤å¸¸è§çš„è¡Œå°¾é—®é¢˜
        cleaned_content = cleaned_content.replace('\r\n', '\n').replace('\r', '\n')

        # ç§»é™¤å°¾éƒ¨çš„ç©ºè¡Œ
        cleaned_content = cleaned_content.strip()

        return cleaned_content

    except Exception as e:
        print(f"âŒ æ–‡ä»¶é¢„å¤„ç†å¤±è´¥: {str(e)}")
        return None


def detect_delimiter(content):
    """ä»å†…å®¹ä¸­æ£€æµ‹åˆ†éš”ç¬¦"""
    if not content:
        return ','

    try:
        # è·å–ç¬¬ä¸€è¡Œ
        first_line = content.split('\n', 1)[0]

        # å¸¸è§åˆ†éš”ç¬¦åŠå…¶å‡ºç°é¢‘ç‡
        delimiters = {
            ',': first_line.count(','),
            '\t': first_line.count('\t'),
            ';': first_line.count(';'),
            '|': first_line.count('|'),
            ' ': first_line.count(' ')  # ç©ºæ ¼åˆ†éš”ç¬¦
        }

        # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„åˆ†éš”ç¬¦
        return max(delimiters, key=delimiters.get)
    except:
        return ','


def safe_strip(value):
    """å®‰å…¨åœ°å»é™¤å­—ç¬¦ä¸²ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦"""
    if value is None:
        return ""
    return str(value).strip()


def convert_to_json(input_file, output_file=None, delimiter=None):
    """
    å°†æ–‡æœ¬æ–‡ä»¶è½¬æ¢ä¸º JSON æ ¼å¼

    å‚æ•°:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è¿”å›åˆ—è¡¨ï¼‰
        delimiter: å­—æ®µåˆ†éš”ç¬¦ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰

    è¿”å›:
        å¦‚æœæœªæŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œåˆ™è¿”å› JSON å¯¹è±¡åˆ—è¡¨
    """
    try:
        # 1. æ£€æµ‹æ–‡ä»¶ç¼–ç 
        encoding = detect_encoding(input_file)
        print(f"ğŸ” æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding}")

        # 2. é¢„å¤„ç†æ–‡ä»¶ï¼ˆç§»é™¤NULå­—ç¬¦ç­‰ï¼‰
        print("ğŸ§¹ æ­£åœ¨æ¸…ç†æ–‡ä»¶ä¸­çš„æ— æ•ˆå­—ç¬¦...")
        content = preprocess_file(input_file, encoding)
        if content is None:
            raise ValueError("æ–‡ä»¶é¢„å¤„ç†å¤±è´¥")

        # 3. æ£€æµ‹åˆ†éš”ç¬¦
        if delimiter is None:
            delimiter = detect_delimiter(content)
            print(f"ğŸ” æ£€æµ‹åˆ°åˆ†éš”ç¬¦: {repr(delimiter)}")

        # 4. ä½¿ç”¨StringIOåˆ›å»ºç±»ä¼¼æ–‡ä»¶çš„å¯¹è±¡
        content_io = StringIO(content)

        # 5. è¯»å–å­—æ®µå
        first_line = content_io.readline()
        if not first_line:
            raise ValueError("æ–‡ä»¶ä¸ºç©ºæˆ–ç¬¬ä¸€è¡Œä¸ºç©º")

        # å®‰å…¨å¤„ç†æ ‡é¢˜è¡Œ
        fieldnames = [safe_strip(field) for field in first_line.split(delimiter)]
        print(f"ğŸ“‹ æ£€æµ‹åˆ°å­—æ®µ: {fieldnames}")

        # 6. é‡ç½®æŒ‡é’ˆ
        content_io.seek(0)

        # 7. åˆ›å»ºCSVå­—å…¸è¯»å–å™¨
        reader = csv.DictReader(content_io, fieldnames=fieldnames, delimiter=delimiter)

        # è·³è¿‡æ ‡é¢˜è¡Œï¼ˆå› ä¸ºæˆ‘ä»¬å·²ç»æ‰‹åŠ¨å¤„ç†äº†ï¼‰
        next(reader, None)

        # 8. æ”¶é›†æ‰€æœ‰è¡Œæ•°æ®
        json_data = []
        line_count = 0

        for row in reader:
            line_count += 1
            try:
                cleaned_row = {}
                for key in fieldnames:
                    # å®‰å…¨å¤„ç†æ¯ä¸ªå€¼
                    value = row.get(key, "")
                    safe_value = safe_strip(value)

                    # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                    try:
                        # å¦‚æœå€¼çœ‹èµ·æ¥åƒæ•°å­—ï¼Œå°è¯•è½¬æ¢
                        if safe_value.replace('.', '', 1).isdigit():
                            if '.' in safe_value:
                                safe_value = float(safe_value)
                            else:
                                safe_value = int(safe_value)
                    except (ValueError, TypeError):
                        pass

                    cleaned_row[key] = safe_value

                json_data.append(cleaned_row)

                if line_count % 1000 == 0:
                    print(f"ğŸ“Š å·²å¤„ç† {line_count} è¡Œæ•°æ®...")

            except Exception as e:
                print(f"âš ï¸ å¤„ç†ç¬¬ {line_count} è¡Œæ—¶å‡ºé”™: {str(e)}")
                print(f"é—®é¢˜è¡Œå†…å®¹: {row}")
                traceback.print_exc()

        print(f"âœ… æˆåŠŸè½¬æ¢ {line_count} è¡Œæ•°æ®")

        # 9. å¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as out_f:
                json.dump(json_data, out_f, indent=4, ensure_ascii=False)
            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            return True
        else:
            return json_data

    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {str(e)}", file=sys.stderr)
        traceback.print_exc()
        return None


def backup_file(file_path):
    """åˆ›å»ºæ–‡ä»¶å¤‡ä»½"""
    backup_path = file_path + ".bak"
    try:
        import shutil
        shutil.copyfile(file_path, backup_path)
        print(f"ğŸ“‚ å·²åˆ›å»ºæ–‡ä»¶å¤‡ä»½: {backup_path}")
        return backup_path
    except Exception as e:
        print(f"âš ï¸ å¤‡ä»½å¤±è´¥: {str(e)}")
        return None


def manual_parse(content, delimiter, fieldnames):
    """æ‰‹åŠ¨è§£æCSVå†…å®¹ï¼ˆå½“æ ‡å‡†æ–¹æ³•å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
    lines = content.split('\n')
    data = []
    line_count = 0

    # è·³è¿‡æ ‡é¢˜è¡Œï¼ˆç´¢å¼•0ï¼‰
    for line in lines[1:]:
        line_count += 1
        try:
            if not line.strip():
                continue

            # å®‰å…¨åˆ†å‰²è¡Œ
            values = [safe_strip(v) for v in line.split(delimiter)]

            # å¦‚æœå­—æ®µæ•°é‡ä¸åŒ¹é…ï¼Œå°è¯•å¤„ç†
            if len(values) != len(fieldnames):
                # å°è¯•æ›´æ™ºèƒ½çš„åˆ†å‰²ï¼ˆå¤„ç†åŒ…å«åˆ†éš”ç¬¦çš„å¼•å·å­—æ®µï¼‰
                values = [safe_strip(v) for v in re.split(f'(?<!\\\\){delimiter}', line)]

                # å¦‚æœä»ç„¶ä¸åŒ¹é…ï¼Œè·³è¿‡æˆ–éƒ¨åˆ†å¤„ç†
                if len(values) > len(fieldnames):
                    values = values[:len(fieldnames)]
                elif len(values) < len(fieldnames):
                    values += [''] * (len(fieldnames) - len(values))

            # åˆ›å»ºè¡Œå­—å…¸
            row = {}
            for i, header in enumerate(fieldnames):
                if i < len(values):
                    row[header] = values[i]
                else:
                    row[header] = ""

            data.append(row)

        except Exception as e:
            print(f"âš ï¸ æ‰‹åŠ¨è§£æç¬¬ {line_count} è¡Œæ—¶å‡ºé”™: {str(e)}")
            print(f"é—®é¢˜è¡Œå†…å®¹: {line}")

    return data


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    input_file = "D:\è°·æ­Œä¸‹è½½æ–‡ä»¶\KEC upload order template(Multi-item Multi line).xlsx"  # æ›¿æ¢ä¸ºæ‚¨çš„æ–‡ä»¶è·¯å¾„

    # è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    output_file = "D:\è°·æ­Œä¸‹è½½æ–‡ä»¶\output.json"

    # åˆ›å»ºæ–‡ä»¶å¤‡ä»½
    backup_file(input_file)

    try:
        # è½¬æ¢æ–‡ä»¶
        result = convert_to_json(
            input_file=input_file,
            output_file=output_file
        )

        # å¦‚æœä¸è¾“å‡ºåˆ°æ–‡ä»¶ï¼Œæ‰“å°ç»“æœ
        if not output_file and result:
            print("è½¬æ¢åçš„ JSON æ•°æ®:")
            print(json.dumps(result, indent=4, ensure_ascii=False))

    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}", file=sys.stderr)

        # å°è¯•æ‰‹åŠ¨è§£æ
        print("ğŸ› ï¸ å°è¯•æ‰‹åŠ¨è§£ææ–‡ä»¶...")
        try:
            encoding = detect_encoding(input_file)
            content = preprocess_file(input_file, encoding)

            if content:
                # æ£€æµ‹åˆ†éš”ç¬¦å’Œå­—æ®µå
                delimiter = detect_delimiter(content)
                first_line = content.split('\n', 1)[0]
                fieldnames = [safe_strip(field) for field in first_line.split(delimiter)]

                # æ‰‹åŠ¨è§£æ
                json_data = manual_parse(content, delimiter, fieldnames)

                if json_data:
                    if output_file:
                        with open(output_file, 'w', encoding='utf-8') as out_f:
                            json.dump(json_data, out_f, indent=4, ensure_ascii=False)
                        print(f"ğŸ’¾ æ‰‹åŠ¨è§£ææ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
                    else:
                        print("æ‰‹åŠ¨è§£æçš„ JSON æ•°æ®:")
                        print(json.dumps(json_data, indent=4, ensure_ascii=False))
                else:
                    print("âŒ æ‰‹åŠ¨è§£æå¤±è´¥ï¼Œæœªç”Ÿæˆæœ‰æ•ˆæ•°æ®")
            else:
                print("âŒ æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹è¿›è¡Œæ‰‹åŠ¨è§£æ")
        except Exception as e2:
            print(f"âŒ æ‰‹åŠ¨è§£æä¹Ÿå¤±è´¥: {str(e2)}")